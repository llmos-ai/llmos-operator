apiVersion: management.llmos.ai/v1
kind: ManagedAddon
metadata:
  name: llmos-gpu-stack
  namespace: llmos-gpu-stack-system
  labels:
    llmos.ai/system-addon: "true"
    llmos.ai/cluster-tools: "true"
  annotations:
    field.llmos.ai/description: "LMOS GPU Stack provides support for virtual GPUs (vGPU) and multi-accelerators, enhancing the utilization and flexibility of GPU resources."
spec:
  repo: http://system-charts-repo.llmos-system.svc
  chart: llmos-gpu-stack
  version: 0.1.1-rc2
  enabled: true
  defaultValuesContent: |-
    gpuStack:
      deviceManager:
        replicas: 1
        image:
          registry: "ghcr.io"
          repository: llmos-ai/llmos-gpu-stack
        service:
          type: ClusterIP
          port: 8080
          profilePort: 6060
        resources:
          requests:
            cpu: 20m
            memory: 50Mi
          limits:
            cpu: 200m
            memory: 512Mi
        tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
            operator: Exists
    crds:
      enabled: true
    gpuOperator:
      enabled: true
    gpu-operator:
      driver:
        enabled: false
      devicePlugin:
        enabled: false
      dcgmExporter:
        enabled: true
        serviceMonitor:
          enabled: false
      operator:
        limits:
          cpu: 500m
          memory: 350Mi
        requests:
          cpu: 50m
          memory: 100Mi
      toolkit:
        env:
          - name: CONTAINERD_CONFIG
            value: /var/lib/rancher/k3s/agent/etc/containerd/config.toml
          - name: CONTAINERD_SOCKET
            value: /run/k3s/containerd/containerd.sock
          - name: CONTAINERD_RUNTIME_CLASS
            value: nvidia
          - name: CONTAINERD_SET_AS_DEFAULT
            value: "true"
      node-feature-discovery:
        resources:
          limits:
            memory: 1024Mi
          requests:
            cpu: 20m
            memory: 50Mi
    volcano:
      enabled: true
      basic:
        image_registry: "ghcr.io"
        controller_image_name: "llmos-ai/mirrored-volcanosh-vc-controller-manager"
        scheduler_image_name: "llmos-ai/mirrored-volcanosh-vc-scheduler"
        admission_image_name: "llmos-ai/mirrored-volcanosh-vc-webhook-manager"
        image_pull_policy: IfNotPresent
      custom:
        scheduler_config_override: |
          actions: "enqueue, allocate, backfill"
          tiers:
          - plugins:
            - name: priority
            - name: gang
              enablePreemptable: false
            - name: conformance
          - plugins:
            - name: overcommit
            - name: drf
              enablePreemptable: false
            - name: deviceshare
              arguments:
                deviceshare.VGPUEnable: true  # enable vgpu
            - name: predicates
            - name: proportion
            - name: nodeorder
            - name: binpack
        admission_resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 20m
            memory: 50Mi
        scheduler_resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 20m
            memory: 50Mi
        controller_resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 20m
            memory: 50Mi
    devicePlugin:
      enabled: true
      image:
        registry: ghcr.io
        repository: llmos-ai/volcano-vgpu-device-plugin
      runtimeClassName: "nvidia"
      nodeSelector:
        gpustack.llmos.ai/gpu-node: "true"
      splitCount: 10
      metrics:
        enabled: false
